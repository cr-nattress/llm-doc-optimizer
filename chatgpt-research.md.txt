Implementation Plan for Document Optimization & Consolidation App
Overview

This plan outlines how to build a Netlify serverless function that uses OpenAI (GPT-4) to process uploaded document(s) into structured, LLM-friendly outputs. The solution will handle single or multiple documents by first optimizing each document individually (preserving truth and structure), then consolidating multiple optimized documents into one master reference. We focus on simplicity (minimal moving parts, clear structure), reliability (robust error handling and testing), fault tolerance (gracefully handling partial failures), and industry standards (security with JWT, clean TypeScript architecture, logging, etc.) throughout the design. Below we detail the approach in five key areas: generic prompt design, JSON output handling, text output handling, security, and GPT-4 integration, followed by additional best practices for a production-ready service.

1. Generic Document Prompt Design (All Documents)

Generalize the prompt templates: We start by analyzing the two existing prompt templates (for individual document optimization and for document consolidation) and refactoring them into generic versions that can apply to any uploaded document(s). The core idea is to remove project-specific references and instead use placeholders or dynamic inserts:

Document Optimization Prompt: A generic prompt instructing the AI to analyze a single document’s content and produce LLM-optimized outputs. It will preserve the document’s truth and wording, normalize the structure (adding headings, IDs, and metadata), and prepare the content for easy consumption by LLMs. All rules from the original (no rewriting content, add stable IDs, include metadata like file name, type, participants, entities, topics, etc.) are retained, but phrased generally (e.g. “Analyze the uploaded document and output a structured Markdown summary with metadata and optional JSON indexes”). This prompt will be used for each document individually.

Document Consolidation Prompt: A generic prompt to merge multiple optimized documents into one unified master document. It instructs the AI to preserve each document’s optimized text exactly, and merge similar sections together (e.g. all policies grouped, transcripts grouped) while maintaining stable section IDs. It also calls for a consolidated Table of Contents at the top and a normalized metadata block for each merged section (listing source files, document type, dates, entities, etc.). We will generalize this prompt so it works for any set of optimized documents by referring to them abstractly (e.g. “Take all optimized documents provided and combine them into a single document with a structured index…”).

These prompt templates will be stored (perhaps in the codebase as template strings or separate files) and parameterized. The system will insert the actual content or references of the uploaded files into the prompt before sending to GPT-4. By keeping the prompts generic and data-driven, the solution can handle any content while ensuring consistent structured outputs. This modular prompt design also allows easy tweaking of instructions for improvements without code changes.

2. JSON Output Handling

Support for JSON outputs: The optimized-document prompt includes an “optional JSON” component for indexes and structured data. Our plan will accommodate scenarios where JSON output is required or beneficial:

We will allow the function’s API to specify an output format mode: for example, "mode": "all" (to get both Markdown and JSON outputs), "mode": "json" (JSON output only), or "mode": "text" (Markdown text output only). By default, the “all” (generic) mode can be used, which will instruct GPT-4 to produce both the markdown content and any JSON index files as specified in the prompt. This ensures maximum information (human-readable and machine-readable) is obtained. If a client only needs structured data (for programmatic use) or only text (for display), they can request those specifically.

Prompt adaptation for JSON: When the "json" mode is requested, the system can adjust the prompt or post-process accordingly. For example, we might modify the prompt to emphasize producing JSON-formatted output (such as skipping the full markdown narrative and just outputting key data in JSON). Similarly, for "text" mode, we could instruct the AI to omit JSON indexes and focus on markdown. This keeps responses concise for the requested format. In “all” mode, the AI will produce the markdown documents along with JSON representations of indexes (e.g. an entity index in both markdown and JSON). We will need to parse and separate these from the AI’s response. To simplify parsing, the prompt can clearly delineate sections (e.g. starting JSON outputs with a marker or in code fences).

Consolidation in JSON form: The consolidation step primarily produces a unified markdown document. If needed, we could also support outputting a consolidated index or metadata in JSON form (e.g. a JSON that lists all sections and their sources). This isn’t explicitly in the original prompt, but as a generic improvement we note it as a possible extension. Initially, the focus will be on the markdown consolidated output since it already contains structured content.

Handling JSON data safely: When the AI returns JSON content, our function will verify that it’s valid JSON (parse it using JSON.parse in a try/catch). If the JSON is malformed (which can happen if the model’s formatting is slightly off), the system will log a warning and either attempt a minor fix or fall back to just returning the markdown output. This ensures robustness when dealing with AI-generated JSON. We will also not expose raw AI output without checks – for instance, ensure any JSON is properly escaped or sanitized if it were to be embedded in a webpage (though in our case it’s likely returned via API, not directly rendered).

In summary, the service will be flexible in output format, catering to both human-readable markdown and machine-readable JSON needs. This is important for downstream integration: e.g., the JSON output could feed another system or UI components, while the markdown is for analysts or indexing by vector databases.

3. Text Output and Processing Flow

Document processing pipeline: The heart of the application is how it ingests documents and produces the optimized text output. We will use a sequential two-stage pipeline to handle single or multiple documents:

Upload & Input Handling: The Netlify function will accept HTTP requests (e.g. a POST) containing the document(s) to process. The request body format could be JSON like { "documents": [ { "name": "...", "content": "..." }, ... ], "mode": "all" }. Each document’s content may be plain text or extracted text from files (for simplicity, we assume the text is provided; any file-to-text conversion would be handled on the client side or a preprocessing step). We will implement input validation: ensure the documents array exists and is not empty, ensure each item has the required fields (name, content), and possibly enforce a size limit per document (to avoid excessively large inputs that could overwhelm the AI or the function’s memory).

Per-Document Optimization: For each document in the input, the function will invoke the OpenAI API with the generic optimization prompt. This prompt will incorporate that document’s content (likely by appending the content after the instructions, or by telling the assistant “here is the text of the document” and including it in the message payload). We might use the OpenAI Chat Completion API with a system message containing the rules (preserve wording, add metadata, etc.), and a user message that provides the document content or any specific user query (here the “task” for the assistant). Each document is processed either sequentially or in parallel depending on performance needs:

To keep things simple and ensure we don’t overload the API, we may start with sequential processing: send the first document, wait for the optimized result, then move to the next. This is reliable and easier to monitor. If performance is a concern (e.g., many documents at once), we could process some in parallel (with care around rate limits and concurrency).

The result for each document will be an optimized markdown text (with stable section IDs, headings, etc.), potentially accompanied by JSON index data if in “all” mode. We will capture these results and store them in memory (e.g., in an array) along with metadata like the document name.

Fault tolerance: If one document’s processing fails (API error or timeout), the system will log the error and continue with the remaining documents. The failed document can be noted in the final output (for instance, we might include a placeholder section stating that document X could not be processed). This way, a single failure doesn’t abort the entire request. We will also implement a reasonable timeout for each OpenAI call (if the API/library doesn’t already have one) to avoid hanging on a single doc.

Consolidation of Multiple Docs: After individual optimization, if multiple documents were provided, the function will perform a consolidation step. It will feed the collection of optimized outputs into the OpenAI API with the consolidation prompt. Concretely, we will concatenate the optimized markdown outputs (or supply them as a list of documents) in the prompt, or use a system message to instruct consolidation and user message listing each optimized doc’s content. The AI (GPT-4) will then return a single master markdown document that merges all inputs according to the rules (grouping similar types, preserving IDs, adding a master table of contents, etc.).

We must ensure the combined content fits within GPT-4’s context window. GPT-4 (especially 32k context version) should handle a reasonable number of documents if they’re not huge. If the total content is too large, an alternative is to consolidate in batches or prompt GPT-4 to summarize each first (but since we already optimized each, they’re likely condensed). For now, we assume typical use (a handful of documents or moderate length) will fit. We will include basic checks on total token size of the prompt and results; if it’s likely to exceed limits, the function can throw an error advising to split the request.

The output from consolidation is expected to be markdown text (with the integrated content). We will capture this result as the final output of the process. If only one document was uploaded, we skip this step and the single optimized document is effectively the final result (the function can just return that).

Returning the Result: The Netlify function will then return an HTTP response to the client. The response structure depends on the requested mode:

In "text" mode, we might return a plain Markdown (text/plain or text/markdown) content of the final output.

In "json" mode, we return JSON data (likely an object containing the indexes or a structured representation of the document’s content). If our prompt was set to produce a JSON summary, we can directly return that parsed JSON. If we always produce markdown then converting to JSON is complex, so using the prompt to directly get JSON is the plan in this mode.

In "all" mode, since multiple formats are produced, we can return a JSON response that contains both: for example, { "markdown": "....", "indexes": { ... } } where the indexes field might be an object with sub-JSON index data. Or simply { "markdown": "... full text ...", "entity_index": [...], "topic_index": [...], ... } depending on how the prompt outputs JSON indexes. Essentially, the function will parse the AI’s response and package it in a clean JSON envelope for the client. This makes the API consumer’s job easier (they don’t have to split the content themselves).

We will also include any relevant metadata in the response if useful (e.g., a list of document names that were processed, maybe processing time, etc.). For error cases (partial or full failures), the response should include an error message or code. For example, if one doc failed but others succeeded, we might return HTTP 207 (Multi-Status) or just 200 with an indication inside the payload that one document was omitted due to an error.

Throughout this flow, text handling is crucial: we are dealing with potentially large text blobs (document contents, AI outputs). We will use streams or efficient string handling for reading the request and writing the response to avoid excessive memory usage (Fastify can handle JSON bodies and our payloads should be fine under typical sizes). We also ensure to preserve text encoding (UTF-8) so that no information (like special characters or formatting from the original documents) is lost when feeding into GPT or when returning results. Overall, this processing flow emphasizes clarity and correctness at each step, making the system reliable and easy to reason about.

4. Security and Authentication (JWT)

Secure API endpoint: Since this function will be invoked from a future website or client, we need to ensure only authorized requests are processed, especially because it will use an OpenAI API key and potentially handle sensitive document data. We will implement JWT-based authentication as follows:

The website (client) will obtain a JWT (JSON Web Token) through a login or identity service (this is outside the scope of this function, but we assume it’s in place). The Netlify function will expect each request to include this JWT, typically in the Authorization header as Bearer <token>.

On each request, the Fastify server (within the function) will validate the JWT. We can use a library like fastify-jwt or the underlying jsonwebtoken to verify the token’s signature and expiration. The token verification will use a secret or public key (depending on whether it’s symmetric or asymmetric JWT) that we configure as an environment variable in Netlify (never hard-coded). For example, we might store JWT_SECRET or JWT_PUBLIC_KEY in Netlify’s settings and have our function load it. This ensures that only clients who possess a valid token (issued by us) can use the endpoint. If verification fails or the token is missing, the function will respond with an HTTP 401 Unauthorized error.

Input validation and sanitization: In addition to auth, we treat the content coming in securely. Even though the content is likely plain text documents, we will guard against malicious inputs. This includes setting a reasonable size limit on request bodies to prevent denial of service. Fastify allows setting a bodyLimit. We might also inspect the content for obvious anomalies (like binary data where text is expected) and reject if it doesn’t meet expected format.

No secret leakage: The OpenAI API key will be kept secure on the server side (as an env var, e.g. OPENAI_API_KEY). The client or user will never see this key. All requests to OpenAI are made from the server, so the key remains hidden. We will also ensure not to log this key or any highly sensitive info.

JWT scope/claims: If needed for extra security, the JWT could carry claims about what the user is allowed to do (e.g. how many documents they can process, or an org ID). Our function could decode the JWT payload (after verifying) and use this info for additional checks (for instance, rate limiting per user or filtering what content can be processed). This is optional, but an industry-standard consideration if this service is multi-tenant or usage-controlled.

TLS and CORS: Netlify provides HTTPS by default, so data in transit is encrypted. We will still ensure that our function only accepts requests from allowed origins (the known website domain) by setting appropriate CORS headers via Fastify’s CORS plugin (or Netlify configuration). This prevents unauthorized JavaScript on other sites from invoking our function even if they somehow have a valid token (or to mitigate certain attacks).

Least privilege & secrets management: In the code, we’ll follow the principle of least privilege. The JWT secret and OpenAI key will be loaded at startup and used only where needed. We won’t hardcode any credentials. Additionally, we will handle errors carefully so as not to leak sensitive info; e.g., if OpenAI API call fails, we return a generic error message rather than the raw response which might contain internal details. All these measures help make the app secure and production-ready.

5. AI Integration (OpenAI GPT-4)

Using OpenAI’s GPT-4 model: At the core of the functionality is the integration with GPT-4, which will generate the optimized documents and consolidated output according to our prompts. Here’s how we will incorporate GPT-4 effectively:

OpenAI API Client: We will use OpenAI’s official Node.js library (openai npm package) or direct HTTPS calls to the OpenAI API. A simple approach is to use the library, instantiate a client with the API key from environment, and call the createChatCompletion or createCompletion method. We’ll specify the model parameter as "gpt-4" (or "gpt-4-32k" if we anticipate very large inputs and have access to that). Using GPT-4 ensures the model can follow the complex instructions in our prompts and handle larger context compared to earlier models.

Prompt Construction: For each stage (optimization and consolidation), the prompt will likely be given as a system message (with the role and the instructions/rules) and possibly a user message containing the actual document content or list of contents. We will carefully construct these messages to fit within token limits. Any placeholders in the generic prompt (like inserting the document text, document name, etc.) will be replaced with the actual content at runtime. It’s important to format the content clearly (perhaps demarcating the document text with delimiters or within triple backticks) so the AI can distinguish instructions from content.

Error handling and retries: When calling GPT-4, various things can go wrong: network issues, API returning an error (rate limit, model overload, etc.), or the model might not follow instructions perfectly. We will implement robust error handling around the API calls:

If the API call fails (throws an exception or returns an error status), we catch it. For a transient error or rate limit (HTTP 429), we might implement a simple retry mechanism: e.g., wait a few seconds and try again up to a couple of times. If it still fails or is a non-recoverable error, we log the error with details and handle it as described (skip that document or return an error response).

If the AI’s response is successfully received but doesn’t meet our format expectations (e.g. missing a JSON part in “all” mode, or content not adhering to rules), we will have to decide on a strategy. In most cases, GPT-4 should follow the prompt if it’s well-designed. We can add checks, for example: verify that certain keywords or headers are present in the output. If something crucial is missing, the function could either post-process to fix minor issues or even call GPT-4 again with a clarification prompt. However, for simplicity and reliability, we prefer to design the prompts clearly up front to minimize this. We will test the prompts with sample documents to fine-tune them during development.

Performance considerations: GPT-4 is powerful but also slower and more costly than smaller models. We will use it because the tasks (deep analysis, index creation) demand its capabilities. To keep performance acceptable:

We ensure to only send necessary content in the prompts (e.g., don’t include irrelevant parts or redundant instructions for each call).

We might use streaming responses for large outputs to start processing sooner, but in a serverless function context, it might be simpler to wait for the full response. Netlify functions have a time limit (usually around 10 seconds for standard functions, and up to a few minutes for background functions). If processing multiple documents with GPT-4, the time could add up. To address this, we could mark the function as a background function if needed (allowing longer execution). Another approach is to impose a limit on number of docs or their length per request to ensure we finish in time.

We will log the time taken for each OpenAI call and overall processing, to monitor performance. If needed, we can optimize by parallelizing as mentioned, or scaling out (Netlify can handle concurrent requests, but each request still limited by its own time).

Configuration: All AI-related configuration (model choice, API keys, possibly temperature or other parameters) will be centralized. For instance, using a config file or environment variables for model name and settings. We likely will use temperature=0 or a low value for consistency, because we want deterministic, factual output (no creative liberties).

Testing with GPT-4: In development, we will test the prompts with sample documents to ensure the outputs follow the rules (e.g., verify that stable IDs are present, metadata is included, content isn’t altered). We might even write unit tests that hit the OpenAI API with a very small dummy input and check structure, although calling the real API in tests is not ideal. Instead, we can mock the OpenAI API responses in tests (provide a fake GPT-4 output) to test our parsing and flow logic. But before going live, several manual test runs with actual GPT-4 will be done to validate the end-to-end chain and adjust prompts as necessary.

By leveraging GPT-4 with careful prompt engineering and rigorous error handling, the AI integration will be reliable and produce high-quality results that meet the specifications of the original prompts.

6. Additional Best Practices and Production Readiness

To ensure the application is maintainable and production-quality, we will incorporate several additional best practices beyond the core logic:

Clean Project Structure: We will organize the code in a standard, logical way for a medium-complexity TypeScript application. For example:

A src/ directory with subfolders like routes/ (for route handlers, though we may only have one route initially), services/ (for business logic like OpenAI API calls and prompt handling), utils/ (for helper functions, e.g. JWT verification if not using a plugin, or text parsing), and config/ (for configuration loading like environment vars).

The Fastify server initialization (if used outside the handler) could be in a file like src/server.ts, and the Netlify function entry point could import this and call fastify.inject or similar to handle the event. Alternatively, since we deploy as a single function, we might simply export an async function handler(event, context) that internally calls the service logic. In either case, code is modular: e.g., a function processDocuments(docs, mode) in the service layer performs the GPT processing given an array of docs.

TypeScript interfaces will be defined for things like the request payload (e.g. interface DocumentRequest { documents: DocumentInput[]; mode: string; }) and for the structured parts of the response (like an interface for the JSON index structure). This provides type safety across the app.

Coding Standards: We will use TypeScript best practices throughout:

Enable strict mode in tsconfig.json (catching undefined issues, etc.).

Use descriptive variable and function names, and write JSDoc or comments for complex logic.

Follow a consistent coding style, possibly enforced by a linter (ESLint) and formatter (Prettier).

Utilize design patterns as appropriate; for instance, a Factory pattern could be used if we later support different AI models or prompt versions (not necessary now, but the code can be written with extensibility in mind). Dependency injection isn’t heavily needed in a simple script, but we might use Fastify’s plugin system to encapsulate things like JWT auth or OpenAI client setup (registering a decorator for the OpenAI client, for example).

Avoid any anti-patterns that can cause memory leaks or slow performance in a serverless environment (like not declaring variables globally which could accidentally persist between invocations—though Netlify might reuse runtime, we assume a fresh run each time for safety).

Logging and Monitoring: Logging is crucial for debugging and maintenance:

We will integrate a structured logging solution. Fastify comes with Pino logger by default, which is high-performance and prints JSON logs. We’ll configure it to log at appropriate levels (info, warn, error). Sensitive data (document content, API keys) will not be logged. Instead, we log meta-information: e.g., “Processed document X, size Y KB, GPT-4 response time 5s” or “OpenAI API error: rate limit exceeded”. This helps in troubleshooting issues in production without exposing private content.

We will add logging especially around error-prone areas: before and after calls to OpenAI (including capturing the duration and any errors), when receiving input (e.g., log number of docs received, and the requesting user’s ID from JWT), and around the consolidation step. If something fails or returns unexpected data, a clear log entry will aid in diagnosing.

In production, these logs can be sent to an aggregation service or viewed via Netlify’s function logs. If needed, we could integrate an alert system (for example, if many errors occur or if a specific error (like authentication failure) happens frequently, we might get notified).

Error Handling & Resilience: We have touched on error handling in logic, but additionally:

The Fastify framework (even in a lambda) can have a global error handler. We will use this to catch any uncaught exceptions and ensure a proper HTTP response is returned rather than a crash. For instance, if something unexpected happens, we return a 500 JSON with { "error": "Internal Server Error" } and log the stack trace. This way, the service fails gracefully.

If an error is due to a known issue (like invalid input), we return a 400 Bad Request with a helpful message. We’ll define clear error messages for scenarios: missing fields, unauthorized, too many documents, etc. This aligns with industry standard API design, making it easier for clients to understand what went wrong.

Fault tolerance measures, as discussed, ensure partial processing continues where possible. In a worst-case scenario where consolidation fails at the end (perhaps due to an unforeseen model error), we might still return the individual optimized documents as a fallback in the response, so the user isn’t left with nothing. This could be included as a contingency in our consolidation prompt handling.

Testing Strategy: We will write unit tests and end-to-end tests to verify the app’s behavior thoroughly:

Unit Tests: Focus on smaller pieces of logic. For example, a unit test for the prompt assembly function (given a sample document content, does it correctly insert into the prompt template?). Another for the response parsing (if GPT returns a combined markdown+JSON, does our parser split it correctly?). We’ll mock external dependencies like the OpenAI API — using dependency injection or by abstracting the OpenAI call behind an interface that we can implement with a mock for tests. We’ll also test utility functions (like JWT verification logic using a known test secret, or any function that transforms data).

Integration/End-to-End Tests: We can simulate HTTP calls to the function to test the whole flow. Fastify has an inject() method to send fake requests directly to the app in-memory, which is great for E2E tests without actually deploying. We will write tests for scenarios such as: a valid request with one document (expect a proper optimized markdown in response), a request with two documents (expect consolidated output with both included), a request with an invalid JWT (expect 401), and edge cases like an extremely large document (which we expect to possibly error or truncate – we can simulate a response or ensure we handle it). We will not actually call the real OpenAI in automated tests (to avoid external dependencies and cost); instead, we might stub the OpenAI client to return a pre-crafted response that looks like what GPT-4 would return (this requires capturing some real examples beforehand). This way, our tests can verify that our code handles the AI output correctly without needing live API calls.

We will also include tests for error paths: for instance, force the OpenAI client mock to throw an error and ensure our function responds with a 500 and does not crash.

Having a good test suite (with coverage especially on the complex logic like prompt handling and result parsing) will give confidence that changes or refactoring won’t break core functionality – a hallmark of production-ready software.

Deployment and DevOps: Since this is a Netlify function, deployment is relatively straightforward (push to a Git repo that Netlify watches). Still, we will ensure a smooth pipeline:

Use a continuous integration (CI) workflow to run the tests and linter on each commit. This can be GitHub Actions or Netlify’s CI. Only deploy if tests pass.

Use environment variables on Netlify for config (OpenAI keys, JWT secrets, etc.) so that secrets are not in code. Document these configurations in a README so developers know how to set up their local env (for local testing we can use a .env file that is not committed).

Monitor the function in production: Netlify provides logs, but for more insight, we could integrate an APM or at least set up notifications for failures. This is optional but a nice production consideration.

Documentation: We will document the API (endpoint path, expected request format, example responses for each mode, auth requirements) for the team or future developers. Also document how to run tests, how to deploy, etc., ensuring maintainability.

By following these practices – a clean modular codebase, strong security, thorough testing, and clear logging – the Node.js app will be simple to maintain, reliable in production, and resilient to issues. In summary, this implementation plan provides a balanced approach prioritizing clarity and robustness, ensuring we meet the project goals with a production-quality solution.